{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dae30af5-010e-45c4-bdea-d25b97dbafcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extracted!\n",
      "ðŸ“° Fake News Sample:\n",
      "                                               title  \\\n",
      "0   Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
      "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
      "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
      "3   Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
      "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
      "\n",
      "                                                text subject  \\\n",
      "0  Donald Trump just couldn t wish all Americans ...    News   \n",
      "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
      "2  On Friday, it was revealed that former Milwauk...    News   \n",
      "3  On Christmas day, Donald Trump announced that ...    News   \n",
      "4  Pope Francis used his annual Christmas Day mes...    News   \n",
      "\n",
      "                date  \n",
      "0  December 31, 2017  \n",
      "1  December 31, 2017  \n",
      "2  December 30, 2017  \n",
      "3  December 29, 2017  \n",
      "4  December 25, 2017  \n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "zip_path = r\"C:\\Users\\melha\\Downloads\\archive.zip\"\n",
    "extract_to = r\"C:\\Users\\melha\\Documents\\fake-news-detector\\data\"\n",
    "\n",
    "# Unzip\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"âœ… Extracted!\")\n",
    "\n",
    "# Load datasets\n",
    "fake_df = pd.read_csv(os.path.join(extract_to, \"Fake.csv\"))\n",
    "real_df = pd.read_csv(os.path.join(extract_to, \"True.csv\"))\n",
    "\n",
    "print(\"ðŸ“° Fake News Sample:\")\n",
    "print(fake_df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "165aaab7-f2bc-4258-bc5b-fef3595e195c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Trump, at NATO, vows unwavering fight against ...      1\n",
      "1  Hacking attacks: a pre-election setback for It...      1\n",
      "2  THE STATE THAT GETS MORE REFUGEES THAN ANY OTH...      0\n",
      "3  Philippines suspends trade with North Korea to...      1\n",
      "4  Libyan force ready to cooperate on UK extradit...      1\n",
      "label\n",
      "0    23481\n",
      "1    21417\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Add labels\n",
    "fake_df['label'] = 0  # fake = 0\n",
    "real_df['label'] = 1  # real = 1\n",
    "\n",
    "# Combine\n",
    "df = pd.concat([fake_df, real_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Combine title + text into one field\n",
    "df['text'] = df['title'] + ' ' + df['text']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ca0837f-16ab-42db-b616-039cf6b375ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\melha\\anaconda3\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\melha\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\melha\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\melha\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\melha\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\melha\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4591ba16-9c74-43a8-95ae-5384571bbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 35918 | Test samples: 8980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label']\n",
    "\n",
    "# 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train samples: {len(X_train)} | Test samples: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc77e04e-1dde-4489-8e36-03b9ac754010",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Use max_features to limit vocab size for speed\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit only on training data\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a686e9e-f375-42b3-b27d-060b4ddf81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4635   71]\n",
      " [  33 4241]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      4706\n",
      "           1       0.98      0.99      0.99      4274\n",
      "\n",
      "    accuracy                           0.99      8980\n",
      "   macro avg       0.99      0.99      0.99      8980\n",
      "weighted avg       0.99      0.99      0.99      8980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ffd2da-9f7c-4b7f-8c62-65058c22ed80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
